{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis with word embedings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data - IMBD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = {'pos': 1, 'neg': 0}\n",
    "\n",
    "train = pd.DataFrame()\n",
    "for sentiment in ('pos', 'neg'):\n",
    "    path =r'aclImdb/train/%s' %(sentiment)\n",
    "    for review_file in os.listdir(path):\n",
    "        with open(os.path.join(path, review_file), 'r') as input_file:\n",
    "            review = input_file.read()\n",
    "        train = train.append([[review, labels[sentiment]]],ignore_index=True)\n",
    "\n",
    "train.columns = ['review', 'Sentiment']\n",
    "indices = train.index.tolist()\n",
    "np.random.shuffle(indices)\n",
    "indices = np.array(indices)\n",
    "train = train.reindex(index=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_iLocIndexer' object has no attribute 'review'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-739ad84a6909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: '_iLocIndexer' object has no attribute 'review'"
     ]
    }
   ],
   "source": [
    "train.iloc(1).review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = {'pos': 1, 'neg': 0}\n",
    "test = pd.DataFrame()\n",
    "for sentiment in ('pos', 'neg'):\n",
    "    path =r'aclImdb/test/%s' %(sentiment)\n",
    "    for review_file in os.listdir(path):\n",
    "        with open(os.path.join(path, review_file), 'r') as input_file:\n",
    "            review = input_file.read()\n",
    "        test = test.append([[review, labels[sentiment]]],ignore_index=True)\n",
    "\n",
    "test.columns = ['review', 'Sentiment']\n",
    "indices = test.index.tolist()\n",
    "np.random.shuffle(indices)\n",
    "indices = np.array(indices)\n",
    "test = test.reindex(index=indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_word_embedings(file):\n",
    "    embeddings = {}\n",
    "    with open(file, 'r') as infile:\n",
    "        for line in infile:\n",
    "            values = line.split()\n",
    "            embeddings[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "    return embeddings\n",
    "\n",
    "fpath = 'glove.6B.300d.txt'\n",
    "embeddings = load_word_embedings(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the libary spacy to tokenize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import re\n",
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Replace breaks, numbers and punctuations\n",
    "def rm_punct(sentence):\n",
    "    sentence = sentence.replace('<br />',\"\")\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) +'0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(' ', sentence)\n",
    "    return nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tokenize with Spacy and remove empty strings\n",
    "my_tok = spacy.load('en')\n",
    "def spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(rm_punct(x)) if tok.text.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def get_non_stopwords(review):\n",
    "    \"\"\"Returns a list of non-stopwords\"\"\"\n",
    "    return {x:1 for x in spacy_tok(str(review).lower()) if x not in stops}.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mean embedding feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_features_mean(s, embeddings=embeddings, emb_size=300):\n",
    "    words = get_non_stopwords(s)\n",
    "    words = [w for w in words if w.isalpha() and w in embeddings]\n",
    "    if len(words) == 0:\n",
    "        return np.hstack([np.zeros(emb_size)])\n",
    "    M = np.array([embeddings[w] for w in words])\n",
    "    return M.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.array([sentence_features_mean(x) for x in train[\"review\"].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array([sentence_features_mean(x) for x in test[\"review\"].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train[\"Sentiment\"].values\n",
    "y_test = test[\"Sentiment\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run XGBOOST with Average Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.679917\ttest-logloss:0.681763\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.412032\ttest-logloss:0.471938\n",
      "[100]\ttrain-logloss:0.326947\ttest-logloss:0.421436\n",
      "[150]\ttrain-logloss:0.281358\ttest-logloss:0.400522\n",
      "[200]\ttrain-logloss:0.251767\ttest-logloss:0.389223\n",
      "[250]\ttrain-logloss:0.229758\ttest-logloss:0.382391\n",
      "[300]\ttrain-logloss:0.212306\ttest-logloss:0.377866\n",
      "[350]\ttrain-logloss:0.196957\ttest-logloss:0.374057\n",
      "[400]\ttrain-logloss:0.184643\ttest-logloss:0.371871\n",
      "[450]\ttrain-logloss:0.173985\ttest-logloss:0.370368\n",
      "[500]\ttrain-logloss:0.164318\ttest-logloss:0.369074\n",
      "[550]\ttrain-logloss:0.155187\ttest-logloss:0.368571\n",
      "[600]\ttrain-logloss:0.147052\ttest-logloss:0.368241\n",
      "Stopping. Best iteration:\n",
      "[596]\ttrain-logloss:0.147687\ttest-logloss:0.368151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb_pars = {\"min_child_weight\": 50, \"eta\": 0.05, \"max_depth\": 8,\n",
    "            \"subsample\": 0.8, \"silent\" : 1, \"nthread\": 4,\n",
    "            \"eval_metric\": \"logloss\", \"objective\": \"binary:logistic\"}\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_test = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'test')]\n",
    "\n",
    "bst2 = xgb.train(xgb_pars, d_train, 800, watchlist, early_stopping_rounds=50, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "veczr = CountVectorizer(tokenizer = spacy_tok, max_features= 10000)\n",
    "train_term_doc = veczr.fit_transform(train['review'])\n",
    "test_term_doc = veczr.transform(test['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.669804\ttest-logloss:0.669906\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-logloss:0.346665\ttest-logloss:0.380222\n",
      "[200]\ttrain-logloss:0.286714\ttest-logloss:0.341599\n",
      "[300]\ttrain-logloss:0.25397\ttest-logloss:0.327673\n",
      "[400]\ttrain-logloss:0.229206\ttest-logloss:0.321084\n",
      "[500]\ttrain-logloss:0.211619\ttest-logloss:0.318603\n",
      "[600]\ttrain-logloss:0.196303\ttest-logloss:0.317063\n",
      "Stopping. Best iteration:\n",
      "[623]\ttrain-logloss:0.193595\ttest-logloss:0.316881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_pars = {\"min_child_weight\": 50, \"eta\": 0.1, \"max_depth\": 8,\n",
    "            \"subsample\": 0.8, \"silent\" : 1, \"nthread\": 4,\n",
    "            \"eval_metric\": \"logloss\", \"objective\": \"binary:logistic\"}\n",
    "\n",
    "d2_train = xgb.DMatrix(train_term_doc, label=y_train)\n",
    "d2_test = xgb.DMatrix(test_term_doc, label=y_test)\n",
    "\n",
    "watchlist = [(d2_train, 'train'), (d2_test, 'test')]\n",
    "\n",
    "bst_bow = xgb.train(xgb_pars, d2_train, 1200, watchlist, early_stopping_rounds=50, verbose_eval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above，BOW approach actually outperforms embedding. However, for the first model, we only used the average embedding of the review, therefore only 300 features, while the BOW has 10000 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
