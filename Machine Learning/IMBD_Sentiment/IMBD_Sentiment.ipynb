{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis with word embedings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data - IMBD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = {'pos': 1, 'neg': 0}\n",
    "\n",
    "train = pd.DataFrame()\n",
    "for sentiment in ('pos', 'neg'):\n",
    "    path =r'aclImdb/train/%s' %(sentiment)\n",
    "    for review_file in os.listdir(path):\n",
    "        with open(os.path.join(path, review_file), 'r') as input_file:\n",
    "            review = input_file.read()\n",
    "        train = train.append([[review, labels[sentiment]]],ignore_index=True)\n",
    "\n",
    "train.columns = ['review', 'Sentiment']\n",
    "indices = train.index.tolist()\n",
    "np.random.shuffle(indices)\n",
    "indices = np.array(indices)\n",
    "train = train.reindex(index=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_iLocIndexer' object has no attribute 'review'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-739ad84a6909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: '_iLocIndexer' object has no attribute 'review'"
     ]
    }
   ],
   "source": [
    "train.iloc(1).review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = {'pos': 1, 'neg': 0}\n",
    "test = pd.DataFrame()\n",
    "for sentiment in ('pos', 'neg'):\n",
    "    path =r'aclImdb/test/%s' %(sentiment)\n",
    "    for review_file in os.listdir(path):\n",
    "        with open(os.path.join(path, review_file), 'r') as input_file:\n",
    "            review = input_file.read()\n",
    "        test = test.append([[review, labels[sentiment]]],ignore_index=True)\n",
    "\n",
    "test.columns = ['review', 'Sentiment']\n",
    "indices = test.index.tolist()\n",
    "np.random.shuffle(indices)\n",
    "indices = np.array(indices)\n",
    "test = test.reindex(index=indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_word_embedings(file):\n",
    "    embeddings = {}\n",
    "    with open(file, 'r') as infile:\n",
    "        for line in infile:\n",
    "            values = line.split()\n",
    "            embeddings[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "    return embeddings\n",
    "\n",
    "fpath = 'glove.6B.300d.txt'\n",
    "embeddings = load_word_embedings(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the libary spacy to tokenize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import re\n",
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Replace breaks, numbers and punctuations\n",
    "def rm_punct(sentence):\n",
    "    sentence = sentence.replace('<br />',\"\")\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) +'0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(' ', sentence)\n",
    "    return nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tokenize with Spacy and remove empty strings\n",
    "my_tok = spacy.load('en')\n",
    "def spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(rm_punct(x)) if tok.text.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def get_non_stopwords(review):\n",
    "    \"\"\"Returns a list of non-stopwords\"\"\"\n",
    "    return {x:1 for x in spacy_tok(str(review).lower()) if x not in stops}.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mean embedding feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_features_mean(s, embeddings=embeddings, emb_size=300):\n",
    "    words = get_non_stopwords(s)\n",
    "    words = [w for w in words if w.isalpha() and w in embeddings]\n",
    "    if len(words) == 0:\n",
    "        return np.hstack([np.zeros(emb_size)])\n",
    "    M = np.array([embeddings[w] for w in words])\n",
    "    return M.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.array([sentence_features_mean(x) for x in train[\"review\"].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.array([sentence_features_mean(x) for x in test[\"review\"].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train[\"Sentiment\"].values\n",
    "y_test = test[\"Sentiment\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run XGBOOST with Average Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.679917\ttest-logloss:0.681763\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.412032\ttest-logloss:0.471938\n",
      "[100]\ttrain-logloss:0.326947\ttest-logloss:0.421436\n",
      "[150]\ttrain-logloss:0.281358\ttest-logloss:0.400522\n",
      "[200]\ttrain-logloss:0.251767\ttest-logloss:0.389223\n",
      "[250]\ttrain-logloss:0.229758\ttest-logloss:0.382391\n",
      "[300]\ttrain-logloss:0.212306\ttest-logloss:0.377866\n",
      "[350]\ttrain-logloss:0.196957\ttest-logloss:0.374057\n",
      "[400]\ttrain-logloss:0.184643\ttest-logloss:0.371871\n",
      "[450]\ttrain-logloss:0.173985\ttest-logloss:0.370368\n",
      "[500]\ttrain-logloss:0.164318\ttest-logloss:0.369074\n",
      "[550]\ttrain-logloss:0.155187\ttest-logloss:0.368571\n",
      "[600]\ttrain-logloss:0.147052\ttest-logloss:0.368241\n",
      "Stopping. Best iteration:\n",
      "[596]\ttrain-logloss:0.147687\ttest-logloss:0.368151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb_pars = {\"min_child_weight\": 50, \"eta\": 0.05, \"max_depth\": 8,\n",
    "            \"subsample\": 0.8, \"silent\" : 1, \"nthread\": 4,\n",
    "            \"eval_metric\": \"logloss\", \"objective\": \"binary:logistic\"}\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_test = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'test')]\n",
    "\n",
    "bst2 = xgb.train(xgb_pars, d_train, 800, watchlist, early_stopping_rounds=50, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "veczr = CountVectorizer(tokenizer = spacy_tok, max_features= 10000)\n",
    "train_term_doc = veczr.fit_transform(train['review'])\n",
    "test_term_doc = veczr.transform(test['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.669804\ttest-logloss:0.669906\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-logloss:0.346665\ttest-logloss:0.380222\n",
      "[200]\ttrain-logloss:0.286714\ttest-logloss:0.341599\n",
      "[300]\ttrain-logloss:0.25397\ttest-logloss:0.327673\n",
      "[400]\ttrain-logloss:0.229206\ttest-logloss:0.321084\n",
      "[500]\ttrain-logloss:0.211619\ttest-logloss:0.318603\n",
      "[600]\ttrain-logloss:0.196303\ttest-logloss:0.317063\n",
      "Stopping. Best iteration:\n",
      "[623]\ttrain-logloss:0.193595\ttest-logloss:0.316881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_pars = {\"min_child_weight\": 50, \"eta\": 0.1, \"max_depth\": 8,\n",
    "            \"subsample\": 0.8, \"silent\" : 1, \"nthread\": 4,\n",
    "            \"eval_metric\": \"logloss\", \"objective\": \"binary:logistic\"}\n",
    "\n",
    "d2_train = xgb.DMatrix(train_term_doc, label=y_train)\n",
    "d2_test = xgb.DMatrix(test_term_doc, label=y_test)\n",
    "\n",
    "watchlist = [(d2_train, 'train'), (d2_test, 'test')]\n",
    "\n",
    "bst_bow = xgb.train(xgb_pars, d2_train, 1200, watchlist, early_stopping_rounds=50, verbose_eval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the aboveï¼ŒBOW approach actually outperforms embedding. However, for the first model, we only used the average embedding of the review, therefore only 300 features, while the BOW has 10000 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
